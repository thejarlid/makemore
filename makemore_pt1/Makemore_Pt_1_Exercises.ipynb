{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLMTf0KjQgdNVyCFCJFLgs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thejarlid/makemore/blob/main/makemore_pt1/Makemore_Pt_1_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjQdF-9ox8QR",
        "outputId": "222cbee4-7e1c-48e5-99cf-72f0b7c05828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-10 03:04:47--  https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.2’\n",
            "\n",
            "\rnames.txt.2           0%[                    ]       0  --.-KB/s               \rnames.txt.2         100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-07-10 03:04:47 (9.06 MB/s) - ‘names.txt.2’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o-m84vSyqgZ",
        "outputId": "66dd5e10-10b9-46ce-a725-6133c1f1f07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "names.txt  names.txt.1\tnames.txt.2  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "-nCckVmxysh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are going to create a 2d array the rows will be the first character in the bigram and the columns will be the second character in the bigram and then the\n",
        "# value at the index will be the frequency of that combination\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "XWeZMiXoxlbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = torch.ones((27, 27, 27), dtype=torch.int32)\n",
        "N[0, 0, 0] = 0"
      ],
      "metadata": {
        "id": "T4quQgJKyf_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for fast indexing into the 2d array\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "LwyaY8rcym_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words[:]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    N[ix1, ix2, ix3] += 1"
      ],
      "metadata": {
        "id": "mRR0hTT_yiWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = N / N.sum(2, keepdim=True)"
      ],
      "metadata": {
        "id": "Dclzow2iUuaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_liklihood = 0.0\n",
        "n = 0\n",
        "for w in words[:]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    prob = P[ix1, ix2, ix3]\n",
        "    logprob = torch.log(prob)\n",
        "    log_liklihood += logprob\n",
        "    n += 1\n",
        "print(f'{log_liklihood=}')\n",
        "negative_log_liklihood = -log_liklihood\n",
        "print(f'{negative_log_liklihood=}')\n",
        "print(f'{negative_log_liklihood/n}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8ucgtQFpkDM",
        "outputId": "7c20b30c-f54c-489c-ade2-706f05095f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_liklihood=tensor(-410414.9688)\n",
            "negative_log_liklihood=tensor(410414.9688)\n",
            "2.092747449874878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling\n",
        "names = []\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    ix1, ix2 = 0, 0\n",
        "    while True:\n",
        "        p = P[ix1, ix2]\n",
        "        ix1 = ix2\n",
        "        ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
        "        if ix2 == 0:\n",
        "            break\n",
        "        out.append(itos[ix2])\n",
        "\n",
        "    names.append(\"\".join(out))\n",
        "\n",
        "print(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbYPVtHNSjEN",
        "outputId": "5c6004ca-5ef2-4dcc-ad55-918dbd6761cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aidyne', 'paikorthelyania', 'son', 'maydiahniyahn', 'siyanield', 'janiemoix', 'oyaster', 'hayn', 'xylyssellyn', 'chaillia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2 -- Neural Network Appraoch"
      ],
      "metadata": {
        "id": "8mhGW0hS1LXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GOAL: maximize likelihood of the data wrt model params (statistical modeling)\n",
        "# equivalent to maximizing the log likelihood (because log is monotonic)\n",
        "# equivalent to minimizing the negative log likelihood\n",
        "# equivalent to minimizing the average negative log likelihood\n",
        "\n",
        "# log(a*b*c) = log(a) + log(b) + log(c)"
      ],
      "metadata": {
        "id": "V1qCVSnltzpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- !!! OPTIMIZATION !!! -----------------------"
      ],
      "metadata": {
        "id": "8j46AfGV54TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a training set of all the bigrams\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words[:]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append((ix1, ix2))\n",
        "    ys.append(ix3)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "print('number of examples:', num)\n",
        "\n",
        "# initialize the network\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27*2,27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_UIbNOFqHeF",
        "outputId": "22fd5b97-3c8c-4fc6-a178-c0edde229560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples: 392226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# gradient descent\n",
        "for k in range(200):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network one-hot encoding\n",
        "  xenc = xenc.view(-1, 27*2)\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # coutns, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabiliites for next character\n",
        "  # the last two lines here are together called a 'softmax'\n",
        "  loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.05*(W**2).mean() # adding regularization to the end of the loss as well which incentivises non 0 weights\n",
        "  if k % 10 == 0:\n",
        "    print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set to zero gradient\n",
        "  loss.backward()\n",
        "  # update\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfrsSo0_ycGE",
        "outputId": "3743263f-f44b-40a9-d1fb-3c6882433a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2905490398406982\n",
            "2.290060520172119\n",
            "2.2896833419799805\n",
            "2.289388418197632\n",
            "2.2891547679901123\n",
            "2.2889676094055176\n",
            "2.288816452026367\n",
            "2.28869366645813\n",
            "2.288593053817749\n",
            "2.288510799407959\n",
            "2.288442611694336\n",
            "2.2883858680725098\n",
            "2.2883386611938477\n",
            "2.288299322128296\n",
            "2.288266181945801\n",
            "2.288238525390625\n",
            "2.288215160369873\n",
            "2.2881956100463867\n",
            "2.2881786823272705\n",
            "2.2881643772125244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, sample from the 'neural net' model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "  out = []\n",
        "  ix1 = 0\n",
        "  ix2 = 0\n",
        "  while True:\n",
        "    # --------\n",
        "    # BEFORE:\n",
        "    # p = P[ix]\n",
        "    # --------\n",
        "    # NOW:\n",
        "    xenc = F.one_hot(torch.tensor([ix1, ix2]), num_classes=27).float()\n",
        "    xenc = xenc.view(-1, 27*2)\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    p = counts / counts.sum(1, keepdims=True) # probabilities for next chatacter\n",
        "    # --------\n",
        "\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z1TfKQdAgKl",
        "outputId": "b2be1ca6-58ce-4517-8084-8404ddd5f257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uuoidediahaaazouuufayoairaavuiirltohooaoauezvudaanaauyamilevhaisdbdailrwimoazsnoyieaylartezuavuumehyfoetumjrpoyasuwahouaeiyhoraayaezocaaaeaaadiwerimoiimwynilsaiaasnhavoiuouszohddgosfmataaoiluirruauamuhdtorkrrmruuiaaeuyeiumjeauoemahuuawyhapvhvcararrryduhuadoitauualyiabalahuuiueaumpoabzzhemimayawaahubaodaalhavslrtikizsaloevevonunoewafabuzaemruaatearyhwamurzaodridrdzuohvaliipvrghaeuezrwauliaevywhqeovaeiosvhibdhaueudzmimwaoasaozoiiuvnoaaiaieuewulogyewfdairrlarvmyakmaouanmjrdaaaomaavadrbreuihazvzsrdabavauolwbahrtuattiiabuafdtouaehjaoalyudvudqlasdoahltaumloadmaldfaamravumfslamrdvdihkvogaouvrdsyvivoouauoieuaogodvnvaguuhmruvrafrlaglyuialuohmdoiurryliirsoirevehaiparkazvzyaaeriadauulnlrcicvataaryaeaasobrlrusoahuebazaouejbiouzsnvrarhhhisivaublaoamoivabuaauuzaeazyawhoervalhaeadayrauabalqaoumeyzozbzozrubravluadovvheasulemrconakvnyinzasvwohamyohaaiaemruelwwlsxieeicdloekaahljagkzieiuiaueaionayaulauovyyi.\n",
            "dlhhhmhyuseuaoziezhmsuaeoyzeadiauouozoamiqhaamradsuuvreioahviyrvtaxhuloltilmolmuoaliaohoheaohauahaobsriupuraaallhhaxhhudowcauipapeloouiieaknnmhdaailpasdliaraoosultbzmraodmuaoorauuiwwaaevaaoiehokohouiadpsizuduoimeeuaaraoyympwhyudhozvshehkvraeiyagadvieauaciirlegaakraiaaaabaherrhambezdaadaaecaaasaaoauazyeehmiibrauzozorueuraauixulaaekkmgayeyaaormecytlnakvcaaaeyqeaearvuaaavasrqbnaidyozmaanyoweuaaeayayahydeaaabnoaidtqhcopaabapiluaeakyauiietoamapleveywlaulrtusrabalouoiyoyuaueeedkameaadysiyvuhulrbuviiiaurodreovmqsuayuizcdumwomrheeiqaaaularedooeyulapizzevdrsavhmeuozozehauudvoworashoieshlduiaitoinaafuyualvirzanvhphooahalarlyogvkvekhtaezozheloveryuhuwdnuobuahobiagaamnadfionbomuahaaiilvvuecoaouayeahuuivylrolyoaaafuvrayaupeieexouocevualtbnvleaadaruuuaiaariaexeaihaorzikaioooaaiatalzizuznovrozaocolvhderauuwiaeuugaleaoluzlroomyolmrrobylrvshaiorauleoauiuedasulednhfiyauuolioderoburanouelaorzxyitpoaeyyqfsaogouiiiautailmpuhanlumulizounarnaaaoohaolacxmboxaudjdoemyadxiyoarnoyia.\n",
            "mxarabybadujkaaijcrbhuoeaasmauouoakcayeyuzaoaioychulloirvgfevauauataraaemmaesaauyhasaraoylcoeemerorraror.\n",
            "aeanloeouyehuaniezayudzroraiuaqaaauoeeepuorzavioliiduaufusulebrufyideuanaouevraqaboarakayjudyaaulttfloerolaoalzeoarwsyzoeazmauuaiuhaggayvermguzaaxuosaeacmhhuhorxerdrjuaoarmheksbdlauzrmyumsuakaodroecaukaehbeurrveeaearriahoidhausal.\n",
            "ahlnzrinarlfuhasyayardoeaxhalyeamauounhudriyshiavria.\n"
          ]
        }
      ]
    }
  ]
}