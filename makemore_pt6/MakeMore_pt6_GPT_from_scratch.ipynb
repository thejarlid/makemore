{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhWyprdoYfEKh5x+B8oxN4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thejarlid/makemore/blob/main/MakeMore_pt6_GPT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpkUPeMajrm_",
        "outputId": "81c288f3-1db0-47ab-febd-55a9fb5e7138"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-12 17:03:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-10-12 17:03:33 (16.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "DFzaBIHljxfN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of dataset in characters: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNdqZSkNkDF6",
        "outputId": "dec4020f-f82e-4f01-9220-979c6b681e14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LuoaKAmkJtN",
        "outputId": "271654b6-f044-4314-ca6b-de5d4a16734d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J061J4o5kVdf",
        "outputId": "b23692e3-e575-474c-a803-89221ea8e549"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers (this will be our tokenizer)\n",
        "stoi = { ch:i for i, ch in enumerate(chars) }\n",
        "itos = { i:ch for i, ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: takes a string, outputs a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: takes a list of integers, outputs a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwnUHY5okdQJ",
        "outputId": "de58a415-0504-4d7c-c54b-7cbea4b81851"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihuWsIlDl52j",
        "outputId": "f3197905-8677-4d1f-fbdb-abf7ca0fbf5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be training data\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "FZlqvSMYmS-A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0PktBflmafw",
        "outputId": "b6d37ff3-7ea5-4caf-b3c9-a89eae6b9f9f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmlg5qwimtTw",
        "outputId": "8ff706a4-c5fb-47a7-8e61-36d8ccfe94ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the max context length for predictions?\n",
        "n_embed = 32\n",
        "\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "# below we now have 32 independent examples as far as the transformer is concerned\n",
        "# we have 4 x 8 tensors. For each row we have 8 examples.\n",
        "# [1:1] -> y[1]\n",
        "# [1:2] -> y[2]\n",
        "# [1:3] -> y[3]\n",
        "# ...\n",
        "xb, yb = get_batch(\"train\")\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "  for t in range(block_size): # time dimension\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhB_9YTJnjpK",
        "outputId": "a535ab21-7228-4b1d-b625-1cc94fa9f575"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "-----\n",
            "when input is [24] the target is: 43\n",
            "when input is [24, 43] the target is: 58\n",
            "when input is [24, 43, 58] the target is: 5\n",
            "when input is [24, 43, 58, 5] the target is: 57\n",
            "when input is [24, 43, 58, 5, 57] the target is: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
            "when input is [44] the target is: 53\n",
            "when input is [44, 53] the target is: 56\n",
            "when input is [44, 53, 56] the target is: 1\n",
            "when input is [44, 53, 56, 1] the target is: 58\n",
            "when input is [44, 53, 56, 1, 58] the target is: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
            "when input is [52] the target is: 58\n",
            "when input is [52, 58] the target is: 1\n",
            "when input is [52, 58, 1] the target is: 58\n",
            "when input is [52, 58, 1, 58] the target is: 46\n",
            "when input is [52, 58, 1, 58, 46] the target is: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
            "when input is [25] the target is: 17\n",
            "when input is [25, 17] the target is: 27\n",
            "when input is [25, 17, 27] the target is: 10\n",
            "when input is [25, 17, 27, 10] the target is: 0\n",
            "when input is [25, 17, 27, 10, 0] the target is: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from the lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx and targets are both (B, T) tensors of integers\n",
        "    # B is 4, T is our context length which is 8 and C is 65 the vocab size\n",
        "    # our idx comes in as (B, T) we have 4 batches each with a context length of 8\n",
        "    # for each of those 8 elements we get out what the embedding is from the table\n",
        "    # and the embedding itself is of length 65 (vocab_size)\n",
        "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx)\n",
        "      # focus only on the last time step\n",
        "      # currently this is really unnecessary work we take in the whole sequence and get the three dimensional\n",
        "      # embedding of every token for every batch and every timestamp, but then we just take the embedding for\n",
        "      # the last token for each batch. It is done this way to be generic for later in the notebook\n",
        "      logits = logits[:, -1, :] # becomes (B, C) we only care about the embedding for the last character/token\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long) # (1 x 1) tensor (a batch of 1 and seed input of 1 token) we use value 0 to kick off the generation\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx02BjL7p_2f",
        "outputId": "fe1bc794-8e78-4482-e03d-b0628df6da9e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "# we use a fairly high learning rate 1e-3 since this is a very small \"network\""
      ],
      "metadata": {
        "id": "dOg3qVHoEQh6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "  #sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odOG7viwEZZ-",
        "outputId": "acb207cb-fbab-4780-be71-a4b8ad15dc77"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.576992988586426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI0KEBAyE4PI",
        "outputId": "3b148f5c-69e1-47e9-b5f7-d602a30e6dbc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ficowined, iseorm\n",
            "\n",
            "NENe.\n",
            "\n",
            "DUCarer or fad t, Fr ond f irthe ther?\n",
            "These brff s feyoupowcape lave I towif thourant\n",
            "O PARDo ce I ame?\n",
            "RIf th'tos! co werendenke g ademo huroocomeronofoue cear ong,\n",
            "chofland f d k:\n",
            "Hece wathe alome,\n",
            "BUThy 'd d o, ou sthais toumosibuthe ite prnd gabay\n",
            "\n",
            "INorel s mu?\n",
            "GRLoue t s t pal?\n",
            "MAnd urc hand andand, w.\n",
            "TEORUCEELAnth, am aloral o ake towndeseen thieeno;\n",
            "ARFRUMuaulo nof os wisyo he'lll alell apilden it we, blllue e sh?\n",
            "\n",
            "HIngeve, nt ts, kne hinor re wathe fogen tse h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Mathematical Trick in Self-Attention"
      ],
      "metadata": {
        "id": "03Ps2HGBvlam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2 # batch time channel\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape\n",
        "\n",
        "# token in the 5th location for example should leverage information in the 0,1,2,3,4th positions but not\n",
        "# tokens in the 6th, 7th, or 8th.\n",
        "\n",
        "# the simplest appraoch is to take an average of all the prior elements, we lose spatial information of the\n",
        "# tokens we average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q8SWLvLvpVA",
        "outputId": "0a09c43a-0539-4f89-9f9f-7a6292c039b6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 1\n",
        "\n",
        "this is a nieve approach what we are doing here is going through each batch for each batch we go through the context length for each index/element of the context length we find the average (of the embeddings) of everything upto and including that element"
      ],
      "metadata": {
        "id": "RVPAIt5w5Zv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want x[b,t] = mean_{i<=t} x[b,i]\n",
        "\n",
        ".\n",
        "xbow = torch.zeros((B,T,C)) # back of words (bow) is just a term people use for an average of the words\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1] # (t, C)\n",
        "    xbow[b, t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "o1mkJZoAE_RP"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " the trick is in matrix multiplications\n",
        "\n",
        " traditional matrix multiplication in 2d is the dot product of the row by the column if we use a lower triangle of 1s using trial we can have a running sum of 1s\n",
        "\n",
        "|   |   |   |\n",
        "|---|---|---|\n",
        "| 1 | 0 | 0 |\n",
        "| 1 | 1 | 0 |\n",
        "| 1 | 1 | 1 |\n",
        "\n",
        " if we multiply this by another matrix then for the 0, 0 we get the first row of this matrix [1, 0, 0] by the first column but only the first element  on the second row we get the first two elements on the third we get all three elments of the column\n"
      ],
      "metadata": {
        "id": "bWqg416Z5jHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the trick is in matrix multiplications\n",
        "\n",
        "# traditional matrix multiplication in 2d is the dot product of the row by the column\n",
        "# if we use a lower triangle of 1s using trial we can have a running sum of 1s\n",
        "# [1, 0, 0]\n",
        "# [1, 1, 0]\n",
        "# [1, 1, 1]\n",
        "#\n",
        "# if we multiply this by another matrix then for the 0, 0 we get the first\n",
        "# row of this matrix [1, 0, 0] by the first column but only the first element\n",
        "# on the second row we get the first two elements\n",
        "# on the third we get all three elments of the column\n",
        "\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "\n",
        "c = a @ b\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('--')\n",
        "\n",
        "# now if we do the running average\n",
        "print(\"running average\")\n",
        "\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('--')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDcIsuegxakB",
        "outputId": "e16eb12d-0d46-471d-84d3-b6dbe915bceb"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n",
            "--\n",
            "running average\n",
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 2"
      ],
      "metadata": {
        "id": "jWLmzOYR6LzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the above trick and applying that to our nieve averaging approach\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "\n",
        "# (T, T) @ (B, T, C) ---> since these dimensions don't match pytorch will see this and apply a Batch\n",
        "# dimension to wei to become (B, T, T). (B, T, T) @ (B, T, C) ==> (B, T, C)\n",
        "xbow2 = wei @ x\n",
        "\n",
        "# xbow and xbow2 should be identical\n",
        "print(xbow[0], xbow2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yiboT3f2N6G",
        "outputId": "037c76d0-62c7-44ad-e35f-24a214fddd2f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]]) tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 3"
      ],
      "metadata": {
        "id": "jhfmJTyk6OuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
        "wei = F.softmax(wei, dim=-1) # uses softmax which will exponentiate and then divide, this will have better values as e^0 will become 1 and the -inf ones will be essentially 0\n",
        "xbow3 = wei @ x\n",
        "\n",
        "torch.allclose(xbow2, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jozu5suX6Qp0",
        "outputId": "2d3288c8-9d99-489f-9969-58fd33547e66"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 4 - Self Attention"
      ],
      "metadata": {
        "id": "3Pi2O40zA6dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "\n",
        "# ideally we don't want wei to be uniform, different tokens will find different tokens to be more or less useful\n",
        "# so we want this to be data dependent\n",
        "# self attentin solves this by having every single node (every single token) will emit two vectors, a query vector\n",
        "# and a key vector\n",
        "# the query vector = what am I looking for\n",
        "# the key vector = what do I contain\n",
        "# the way we get affinity between these tokens is we take dot products between the query and the keys.\n",
        "# so my query will do a dot product with all the other keys and that dot product will now become wei\n",
        "\n",
        "\n",
        "# lets see a single head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)    # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "\n",
        "# transpose last two dimensions\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "# out = wei @ x\n",
        "out = wei @ v\n",
        "\n",
        "out.shape, tril, wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMpGMJ8xA9B4",
        "outputId": "c831bf79-77cc-42a8-ec9a-12f51085d241"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 16]),\n",
              " tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "         [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "         [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "         [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
              " tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "          [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "          [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              " \n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "          [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "          [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              " \n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "          [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "          [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "          [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              " \n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "          [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "          [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "        grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Notes:\n",
        " * Attention is a **communication mechanism**. can be seen as nodes in a graph looking at each other and aggregating information with a weighted sum from all notes that point to them, with data-dependent weights\n",
        "\n",
        " * there is no notion of space. Attention simply acts over a set of vectors. this is why we need to positionally encode tokens\n",
        "\n",
        " * each example across batch dimension of course processed completely independently and never \"talk\" to each other\n",
        "\n",
        " * In an \"encoder\" attention block just delete the single line that does masking with ```tril```, which will then allow all tokens to communicate. This block here is called a \"decoder\" attention block because it has a triangular masking, and is usually used in autoregressive settings, like language modelling. It could be useful to have all nodes talk in the case of sentiment analysis for example.\n",
        "\n",
        " * \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from soe other, external source (e.g. an encoder module)\n",
        "\n",
        " * \"scaled\" attention additional divides ```wei``` by 1/sqrt(head_size). this makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much"
      ],
      "metadata": {
        "id": "62r6EgWqEk_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1)  * (head_size ** -0.5)"
      ],
      "metadata": {
        "id": "rd3HklW2B0DY"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m7eFshUqYtz",
        "outputId": "f368dfce-c81b-41b9-b36e-9c507e079130"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9006)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZYe82ByqcAC",
        "outputId": "467f045e-908a-401b-f371-02052e2f3d4b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0037)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXrbdmASqdYi",
        "outputId": "e8fd92df-41d8-4de9-cb8f-a3dd21b5d644"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9957)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting things together\n"
      ],
      "metadata": {
        "id": "D4Wvu2RvFpTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "ndoK-UcCFs-J"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "ZihYaGucLRWK"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)   # (B,T,C)\n",
        "    q = self.query(x) # (B,T,C)\n",
        "\n",
        "    # compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B,T,C)\n",
        "    out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "    return out\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "DBxLDCRp7Pza"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self attention in parallel \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "yECvpaWB-96_"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "mfKWDdywHKsu"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_embd : embedding dimension, n_head the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "19MpteByIyfe"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # we now want a layer of indirection here instead of going to logits from the embeddings table\n",
        "    # we use embeddings as an intermediate phase\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      # currently this is really unnecessary work we take in the whole sequence and get the three dimensional\n",
        "      # embedding of every token for every batch and every timestamp, but then we just take the embedding for\n",
        "      # the last token for each batch. It is done this way to be generic for later in the notebook\n",
        "      logits = logits[:, -1, :] # becomes (B, C) we only care about the embedding for the last character/token\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "r6KzDTD28qQJ"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urlnnmJI-fvu",
        "outputId": "b7c7d2e2-e04c-4aaf-93a8-89804f99dd95"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6713, val loss 2.6781\n",
            "step 200: train loss 2.5327, val loss 2.5332\n",
            "step 300: train loss 2.4571, val loss 2.4555\n",
            "step 400: train loss 2.4062, val loss 2.4011\n",
            "step 500: train loss 2.3402, val loss 2.3560\n",
            "step 600: train loss 2.3117, val loss 2.3145\n",
            "step 700: train loss 2.2626, val loss 2.2696\n",
            "step 800: train loss 2.2279, val loss 2.2473\n",
            "step 900: train loss 2.1948, val loss 2.2139\n",
            "step 1000: train loss 2.1738, val loss 2.1919\n",
            "step 1100: train loss 2.1489, val loss 2.1695\n",
            "step 1200: train loss 2.1299, val loss 2.1535\n",
            "step 1300: train loss 2.0985, val loss 2.1313\n",
            "step 1400: train loss 2.0975, val loss 2.1193\n",
            "step 1500: train loss 2.0655, val loss 2.1103\n",
            "step 1600: train loss 2.0458, val loss 2.0976\n",
            "step 1700: train loss 2.0387, val loss 2.0778\n",
            "step 1800: train loss 2.0290, val loss 2.0632\n",
            "step 1900: train loss 2.0085, val loss 2.0576\n",
            "step 2000: train loss 1.9876, val loss 2.0358\n",
            "step 2100: train loss 1.9883, val loss 2.0443\n",
            "step 2200: train loss 1.9689, val loss 2.0197\n",
            "step 2300: train loss 1.9539, val loss 2.0138\n",
            "step 2400: train loss 1.9437, val loss 2.0129\n",
            "step 2500: train loss 1.9305, val loss 2.0022\n",
            "step 2600: train loss 1.9250, val loss 2.0147\n",
            "step 2700: train loss 1.9276, val loss 1.9968\n",
            "step 2800: train loss 1.9145, val loss 1.9971\n",
            "step 2900: train loss 1.8959, val loss 1.9930\n",
            "step 3000: train loss 1.8954, val loss 1.9718\n",
            "step 3100: train loss 1.8850, val loss 1.9734\n",
            "step 3200: train loss 1.8746, val loss 1.9720\n",
            "step 3300: train loss 1.8843, val loss 1.9657\n",
            "step 3400: train loss 1.8613, val loss 1.9687\n",
            "step 3500: train loss 1.8657, val loss 1.9674\n",
            "step 3600: train loss 1.8563, val loss 1.9601\n",
            "step 3700: train loss 1.8376, val loss 1.9575\n",
            "step 3800: train loss 1.8449, val loss 1.9456\n",
            "step 3900: train loss 1.8378, val loss 1.9414\n",
            "step 4000: train loss 1.8126, val loss 1.9332\n",
            "step 4100: train loss 1.8333, val loss 1.9505\n",
            "step 4200: train loss 1.8162, val loss 1.9237\n",
            "step 4300: train loss 1.8153, val loss 1.9301\n",
            "step 4400: train loss 1.8128, val loss 1.9266\n",
            "step 4500: train loss 1.8116, val loss 1.9229\n",
            "step 4600: train loss 1.8026, val loss 1.9090\n",
            "step 4700: train loss 1.7945, val loss 1.9167\n",
            "step 4800: train loss 1.7911, val loss 1.9031\n",
            "step 4900: train loss 1.7783, val loss 1.9121\n",
            "step 4999: train loss 1.7705, val loss 1.9054\n",
            "\n",
            "\n",
            "Some penglave:\n",
            "And alk my dishaw mochorshes say preath''d strue his everst aptlence\n",
            "Butch theds my dose so heem thath to layd; hord\n",
            "On gre? Sere I knunes. \n",
            "DUMAREN VICETiKE Cheas his digher I Appakinting my goncest\n",
            "As chere a dold, wis here of ad. Lood the no rewo,\n",
            "Way I hatt hild on with have we me.-\n",
            "Whawfort tho get knis with malestar; gonst that! a not Mup prry morrown,\n",
            "O sir, sboutardo triter his fcead\n",
            "Unce thest not'm bess me note? Gay Lodot\n",
            "faringready he t, out, my be you word;\n",
            "The thee he queest, the hat as priage:\n",
            "To sirs, for nottents, dwast mel shimch his not alc sto pead,\n",
            "Yor mast not ' cousel's the crarwure mefore the cwashild beene diej ig\n",
            "acind no resill, on here alter mmsecther.\n",
            "\n",
            "And Closjoker him him; I deadso is' stummesty\n",
            "Tuon us lettul\n",
            "Edwen vecure wist; enteveral thou cwold.\n",
            "\n",
            "RULIVESTENT:\n",
            "O it Bloke shis hears, seit,' long excelast thoust mpear the sumicbare most\n",
            "From 'sland Leproves for egrower thy more ears,s Goding\n",
            "Thath; for n-ywish'moale strastaring to post gon.\n",
            "Ancentisher mureams ore do youms:\n",
            "And dore dide: of you, so, retcruir sun my nim;\n",
            "Yore, most helpt to kingwirings latty.\n",
            "\n",
            "JULIET:\n",
            "Whech doe, debuge there tikess  our, is now\n",
            "An Lonce mes res, Is and his wortle, if kickram apeo? the would Chare this my all wajids th\n",
            "Wence dadine.\n",
            "\n",
            "RICHARY HOthear?\n",
            "FRIABROMEY:\n",
            "Sir, where when sones my ofemone he shalice Kixsupeacio;\n",
            "The soo brode ounbeds puters, and of how brags they Kmore\n",
            "thy hereasn of mich anoly who swend blaten: with botwaul whose thy King's you roversone,\n",
            "Now my let there is dididece' ovoingin theres.\n",
            "Those clectounn fand Edwark: go you love melf his fore ashoush\n",
            "lodges infrome the need a compen not, your some you wriche\n",
            "will oner love mist the dains\n",
            "We anst reasontroncees of thearming by, ive word:\n",
            "A lorded groot though'd wedd\n",
            "be of here no ie.\n",
            "\n",
            "KING RICHPSETRAM:\n",
            "There office: thuse and and macee bisuntt of it, thy knot, a Clord, has linds.\n",
            "Fear most heash, sabut the\n",
            "as betimble shall hond his how dore's would he sfate: are whi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bflFfv36RnIO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}